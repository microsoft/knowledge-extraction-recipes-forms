{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Boxes Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:38:48.444008Z",
     "start_time": "2020-06-02T14:38:48.398391Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to remove boxes from a form as these boxes can cause OCR misreads if the text overlaps with the boundaries of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:38:49.716233Z",
     "start_time": "2020-06-02T14:38:49.188141Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\".\")\n",
    "import matplotlib.pyplot as plt\n",
    "from requests import get, post, delete\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "__root_common__ = 'common.py'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__root_common__), '..')))\n",
    "\n",
    "__train_file__ = 'autolabel_training.py'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__train_file__), '..')))\n",
    "\n",
    "import numpy as np\n",
    "from boxdetect import config\n",
    "from common.common import compute_partial_ratio, compute_ratio, get_text_from_ocr, score_and_rank, \\\n",
    "apply_erosion, apply_dilatation, get_projection, load_image, find_runs, analyze_runs\n",
    "from Training.Auto_Labelling.basic_implementation.autolabel_training import call_ocr\n",
    "\n",
    "best_score = {}\n",
    "\n",
    "config.thickness = 3\n",
    "DATA_PATH = '../Data/'\n",
    "file_name = 'Gedoc_1 (2).tiff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:38:50.918016Z",
     "start_time": "2020-06-02T14:38:50.869304Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaling_factors param is the one that affects performance the most\n",
    "# - it defines all the scaling factors that should be used while processing to search for rectangles\n",
    "# - the more and more diverse the better \n",
    "# - smaller value -> faster processing time\n",
    "# - going below 0.4 is risky because image can turn blury and might generate more false positives\n",
    "print(\"Default scaling factors: \", config.scaling_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we autodetect boxes on the form - this is a poor quality image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:39:36.278831Z",
     "start_time": "2020-06-02T14:38:54.231490Z"
    }
   },
   "outputs": [],
   "source": [
    "from boxdetect.pipelines import process_image\n",
    "\n",
    "rects, grouping_rects, img, output_image = process_image(\n",
    "    os.path.join(DATA_PATH, file_name), config=config, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each rectangle is a big region of box rectangles\n",
    " (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:39:36.345786Z",
     "start_time": "2020-06-02T14:39:36.280675Z"
    }
   },
   "outputs": [],
   "source": [
    "grouping_rects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what has been detected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:39:37.815825Z",
     "start_time": "2020-06-02T14:39:36.349787Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,25))\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here you can interactively extract each section that the detect boxes has identified - use this to tune the sensitivity of the region extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(i=(0, len(grouping_rects)-1))\n",
    "def extract_region(i):\n",
    "    \n",
    "    window1 = 1   # Tweak these sensitivity values\n",
    "    window2 = 0.01  # Tweak these sensitivity values\n",
    "    \n",
    "    y1 = int(grouping_rects[i][1] + (window1 * grouping_rects[i][3]))   \n",
    "    y2 = int(grouping_rects[i][1] - (window2 * grouping_rects[i][3]))\n",
    "    x1 = int(grouping_rects[i][0] + (window1 * grouping_rects[i][2]))\n",
    "    x2 = int(grouping_rects[i][0] - (window2 * grouping_rects[i][2]))\n",
    "    roi = output_image[y2:y1, x2:x1]\n",
    "    plt.imshow(roi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's drilldown on the City field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the values for window1 and window2 below to fit the region snugly with the detected boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window1 = 1   # Tweak these sensitivity values\n",
    "window2 = 0.01  # Tweak these sensitivity values\n",
    "\n",
    "# Let's take the city field as an example\n",
    "i = 1\n",
    "    \n",
    "y1 = int(grouping_rects[i][1] + (window1 * grouping_rects[i][3]))   \n",
    "y2 = int(grouping_rects[i][1] - (window2 * grouping_rects[i][3]))\n",
    "x1 = int(grouping_rects[i][0] + (window1 * grouping_rects[i][2]))\n",
    "x2 = int(grouping_rects[i][0] - (window2 * grouping_rects[i][2]))\n",
    "roi = output_image[y2:y1, x2:x1]\n",
    "\n",
    "# Let's save the image\n",
    "image = cv2.imread(os.path.join(DATA_PATH, file_name))\n",
    "roi = image[y2:y1, x2:x1]\n",
    "active_file = 'city.jpg' \n",
    "saved = cv2.imwrite(os.path.join(DATA_PATH, active_file), roi)\n",
    "\n",
    "assert saved == True\n",
    "print(f\"{DATA_PATH + active_file} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will set up some of the environment variables here and others later in the notebook to keep things clear\n",
    "# Set the values here marked with SET THIS HERE\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Read from .env file\n",
    "    \"\"\"\n",
    "    REGION = 'eastus'  # The region Form Recognizer and OCR are deployed\n",
    "    SUBSCRIPTION_KEY = ''  # CogSvc key frautolabel | Keys and Endpoint\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's call OCR and score against the Ground Truth - Enter the GT here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = 'ADD YOUR VALUE HERE'    # This is the ground truth value for the field\n",
    "result = call_ocr(DATA_PATH, active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "best_score, top_score = score_and_rank(active_file, GT, result, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get our baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_score = top_score[0][1]\n",
    "print(f\"Baseline score {baseline_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's find the best performing image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(os.path.join(DATA_PATH, active_file))\n",
    "gray = cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "# Remove vertical\n",
    "vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,25))\n",
    "detected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n",
    "cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    cv2.drawContours(image, [c], -1, (255,255,255), 2)\n",
    "    \n",
    "# Repair image\n",
    "repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,6))\n",
    "result = 255 - cv2.morphologyEx(255 - image, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\n",
    "\n",
    "saved = cv2.imwrite(os.path.join(DATA_PATH, 'thresh.jpg'), thresh)\n",
    "assert saved == True\n",
    "print(f\"{DATA_PATH + 'thresh.jpg'} saved\")\n",
    "saved = cv2.imwrite(os.path.join(DATA_PATH, 'detected.jpg'), detected_lines)\n",
    "assert saved == True\n",
    "print(f\"{DATA_PATH + 'detected.jpg'} saved\")\n",
    "saved = cv2.imwrite(os.path.join(DATA_PATH, 'roi.jpg'), roi)\n",
    "assert saved == True\n",
    "print(f\"{DATA_PATH + 'roi.jpg'} saved\")\n",
    "\n",
    "\n",
    "active_file = 'roi.jpg'\n",
    "result = call_ocr(DATA_PATH, active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "best_score, top_score = score_and_rank(active_file, GT, result, best_score)\n",
    "\n",
    "inverted_active_file = 'thresh.jpg'\n",
    "result = call_ocr(DATA_PATH, inverted_active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "best_score, top_score = score_and_rank(active_file, GT, result, best_score)\n",
    "\n",
    "for i in range(2):\n",
    "    i+=1\n",
    "\n",
    "    active_file = apply_dilatation(DATA_PATH, active_file, i)\n",
    "    result = call_ocr(DATA_PATH, active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "    best_score, top_score = score_and_rank(active_file, GT, result, best_score)\n",
    "\n",
    "    active_file = apply_erosion(DATA_PATH, active_file, i)\n",
    "    result = call_ocr(DATA_PATH, active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "    best_score, top_score = score_and_rank(active_file, GT, result, best_score)\n",
    "\n",
    "    active_file = apply_erosion(DATA_PATH, inverted_active_file, i)\n",
    "    result = call_ocr(DATA_PATH, active_file, 'en', Config.REGION, Config.SUBSCRIPTION_KEY, 'image/jpeg')\n",
    "    best_score, top_score = score_and_rank(inverted_active_file, GT, result, best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline score {baseline_score}\")\n",
    "print(f\"Best performing image {top_score[0][0]} {top_score[0][1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
