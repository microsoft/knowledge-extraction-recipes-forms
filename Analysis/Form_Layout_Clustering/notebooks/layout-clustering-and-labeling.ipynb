{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting autoreload to automatically pickup changes in local packages imported from `../src` directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T15:20:43.362305Z",
     "start_time": "2020-04-23T15:20:43.320309Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T15:20:46.372792Z",
     "start_time": "2020-04-23T15:20:44.013245Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import ipyplot\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from df_helpers import (\n",
    "    create_dataframe_from_files,\n",
    "    create_dataframe_from_pd,\n",
    "    read_ocr_from_json,\n",
    "    clean_text_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe\n",
    "\n",
    "We assume that pdfs have been already converted into images and ocr'd with tesseract.    \n",
    "Output of all these operations (.PDF/.PNG/.JSON) should ideally be under a single folder in local env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T11:45:33.154736Z",
     "start_time": "2020-04-15T11:45:02.018272Z"
    }
   },
   "source": [
    "#### Use code below to create dataframe based on the files present in the directory\n",
    "```python\n",
    "df_raw = create_dataframe_from_files(\n",
    "    data_dir=\"../../../../datasets/500k/equal_split\",\n",
    "    file_ext='.png',\n",
    "    first_page_only=True,\n",
    "#     load_ground_truth=\"../output/df_5sup_pred.csv\",\n",
    "    dropna=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use code below to create a datframe based on an existing DF (loaded from csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:20:48.435755Z",
     "start_time": "2020-04-24T10:20:46.390092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows count before removing NaN values:  5100\n",
      "Rows count after removing NaN values:  5100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../datasets/800/MSFT_MVP_Field_Output_Sample800.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df_raw = create_dataframe_from_pd(\n",
    "    (\n",
    "        df[\n",
    "            df.groupby(['ProviderName'])['FileName']\n",
    "            .transform('count') >= 201 # select a specific subset of providers with more than 200 invoices\n",
    "        ]\n",
    "#         [\n",
    "#             df.groupby(['ProviderName'])['FileName']\n",
    "#             .transform('count') > 50 # create a lower boundry for number of invoices per provider\n",
    "#         ]\n",
    "        .groupby(['ProviderName'])\n",
    "        .head(150)\n",
    "#         .apply(lambda x: x.sample(50, replace=False)) # take N randomly sampled invoices for each provider\n",
    "    ),\n",
    "    data_dir=\"../../../datasets/800/data_all\" # point out the directory where files are located\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:20:49.485956Z",
     "start_time": "2020-04-24T10:20:49.375987Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading OCR + cleaning/filtering text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:21:28.472686Z",
     "start_time": "2020-04-24T10:20:56.801703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty texts:  55\n",
      "Total rows count:  5045\n"
     ]
    }
   ],
   "source": [
    "# read ocr data from json file for each row\n",
    "df_ocr = read_ocr_from_json(\n",
    "    df_raw)\n",
    "# clean ocr text for each row\n",
    "df_clean = clean_text_column(\n",
    "    df_ocr,\n",
    "    chars_regex=\"[a-zA-Z]+\",\n",
    "    keep='all',\n",
    "    min_words_count=20)\n",
    "# df_clean = df_clean.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:28:25.336290Z",
     "start_time": "2020-04-24T10:24:48.688776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\Anaconda3\\envs\\py36tf2gpu\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from text_features import fuzzy_replace\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "\n",
    "with open(\"./invoice_vocabulary.txt\", 'r') as file:\n",
    "    invoice_vocabulary = file.read().replace('\\n', ' ').split()    \n",
    "\n",
    "all_text = df_clean['TextClean'].values\n",
    "\n",
    "# logical=True counts threads, but we are interested in cores\n",
    "max_pool = 20\n",
    "pool = Pool(max_pool)\n",
    "pool_outputs = pool.map(\n",
    "    partial(\n",
    "        fuzzy_replace,\n",
    "        query_list = invoice_vocabulary,\n",
    "        threshold=85,\n",
    "        whitelist=True),\n",
    "    all_text\n",
    ")\n",
    "pool.close()\n",
    "pool.join()\n",
    "pool.terminate()\n",
    "\n",
    "df_clean['TextWhite'] = pool_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:28:25.446296Z",
     "start_time": "2020-04-24T10:28:25.338314Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:28:25.546287Z",
     "start_time": "2020-04-24T10:28:25.447286Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for clustering/labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:35:29.952400Z",
     "start_time": "2020-04-24T10:35:29.859393Z"
    }
   },
   "outputs": [],
   "source": [
    "SELECTED_SUPPLIER = 'new'\n",
    "\n",
    "#TFIDF\n",
    "MAX_FEAT = 500\n",
    "NGRAM_RANGE = (2, 4)\n",
    "\n",
    "#PCA\n",
    "USE_PCA = True\n",
    "PCA_COMP = 30\n",
    "\n",
    "#Scaler\n",
    "USE_SCALER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cleaned text through TFIDFProcessor to generate features vectors\n",
    "Depending on params it may or may not use TFIDFVectorizer, PCA and StandardScaler transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:35:31.862472Z",
     "start_time": "2020-04-24T10:35:30.852023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF shape: (5045, 500)\n",
      "PCA shape: (5045, 30)\n"
     ]
    }
   ],
   "source": [
    "from text_features import TFIDFProcessor\n",
    "\n",
    "tfidf_proc = TFIDFProcessor(\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    max_feat=MAX_FEAT,\n",
    "    use_pca=USE_PCA,\n",
    "    pca_components=PCA_COMP,\n",
    "    use_scaler=USE_SCALER,\n",
    "    verbose=2,\n",
    ")\n",
    "# df_clean = df_clean['LayoutType'].values != '-1'\n",
    "# X = df_clean[\"TextClean\"].values\n",
    "X = df_clean[\"TextWhite\"].values\n",
    "# IMG_PATHS = np.asarray([path.replace('\\\\','/')[3:] for path in df_clean[\"ImagePath\"].values])\n",
    "IMG_PATHS = df_clean[\"ImagePath\"].values\n",
    "X_feat = tfidf_proc.fit_pipeline(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply clustering with DBSCAN alg\n",
    "**eps** - element-wise distance threshold to determine if elements are close enough to be in the same cluster. The higher the final features size (PCA components number) the higher eps value should be   \n",
    "**min_samples** - min samples in the same \"neighborhood\" to be considered a cluster. The higher this value will be the more well represented layout types we will pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:36:00.000088Z",
     "start_time": "2020-04-24T10:35:58.778826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 39\n",
      "Estimated number of noise points: 508\n"
     ]
    }
   ],
   "source": [
    "from clustering import fit_dbscan\n",
    "\n",
    "labels, dbscan = fit_dbscan(X_feat, eps=0.30, min_samples=35, plot=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T12:58:24.204374Z",
     "start_time": "2020-04-23T12:58:24.116349Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is how to sideload and reuse previously known layout types (if present)\n",
    "put_mask = df_clean['LayoutType'].values != '-1'\n",
    "labels[put_mask] = df_clean['LayoutType'].values[put_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:36:02.647615Z",
     "start_time": "2020-04-24T10:36:02.000463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster -1 count:  508\n",
      "Cluster 00 count:  242\n",
      "Cluster 01 count:  117\n",
      "Cluster 02 count:  149\n",
      "Cluster 03 count:  136\n",
      "Cluster 04 count:  379\n",
      "Cluster 05 count:  297\n",
      "Cluster 06 count:  136\n",
      "Cluster 07 count:  135\n",
      "Cluster 08 count:  142\n",
      "Cluster 09 count:  113\n",
      "Cluster 10 count:  42\n",
      "Cluster 11 count:  131\n",
      "Cluster 12 count:  100\n",
      "Cluster 13 count:  129\n",
      "Cluster 14 count:  57\n",
      "Cluster 15 count:  136\n",
      "Cluster 16 count:  127\n",
      "Cluster 17 count:  79\n",
      "Cluster 18 count:  137\n",
      "Cluster 19 count:  146\n",
      "Cluster 20 count:  143\n",
      "Cluster 21 count:  86\n",
      "Cluster 22 count:  91\n",
      "Cluster 23 count:  104\n",
      "Cluster 24 count:  83\n",
      "Cluster 25 count:  119\n",
      "Cluster 26 count:  107\n",
      "Cluster 27 count:  138\n",
      "Cluster 28 count:  58\n",
      "Cluster 29 count:  146\n",
      "Cluster 30 count:  142\n",
      "Cluster 31 count:  50\n",
      "Cluster 32 count:  64\n",
      "Cluster 33 count:  36\n",
      "Cluster 34 count:  72\n",
      "Cluster 35 count:  40\n",
      "Cluster 36 count:  53\n",
      "Cluster 37 count:  37\n",
      "Cluster 38 count:  38\n"
     ]
    }
   ],
   "source": [
    "# lists detected clusters + value counts for each\n",
    "for i in np.unique(labels):\n",
    "    print('Cluster %s count: ' % i, sum(labels == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:14:23.821556Z",
     "start_time": "2020-04-22T14:14:23.725564Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this to change the label for all elements from specific cluster to another label\n",
    "labels[labels=='11'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T11:31:50.608826Z",
     "start_time": "2020-04-17T11:31:50.512916Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this to change labels for specific elements from a specific cluster\n",
    "from clustering import change_labels\n",
    "\n",
    "labels = change_labels(\n",
    "    labels.copy(), \n",
    "    cluster_name='16',\n",
    "    idx_to_change=[1], \n",
    "    target_labels=['-1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting with IPyPlot package for (much) better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:36:06.260151Z",
     "start_time": "2020-04-24T10:36:05.746964Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print top N samples for each cluster in a separate interactive tab\n",
    "ipyplot.plot_class_tabs(IMG_PATHS, labels, max_imgs_per_tab=10, img_width=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T15:07:50.740229Z",
     "start_time": "2020-04-23T15:07:50.649243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print top N samples for specific cluster\n",
    "ipyplot.plot_images(IMG_PATHS[labels=='10'], max_images=20, img_width=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T10:36:09.262556Z",
     "start_time": "2020-04-24T10:36:09.171558Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print first element from each cluster\n",
    "# takes in labels and coresponding IMG_PATHS\n",
    "ipyplot.plot_class_representations(IMG_PATHS, labels, img_width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:24:43.055287Z",
     "start_time": "2020-04-22T14:24:42.891362Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean['LayoutType'] = labels\n",
    "df_clean['LayoutType'] = df_clean['LayoutType'].apply(lambda x: x if (x == '-1') else 'layout_type_' + x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36tf2gpu]",
   "language": "python",
   "name": "conda-env-py36tf2gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}